NiNnet

调参记录：
关于学习率：

官方设定 lr = 0.1:
    收敛正常，训练正常。
    
调整为原来的10倍：lr = 1.0，0.5，0.3
    不会收敛，一直停留在第一轮的loss上
    
调整为原来的20倍：lr = 2.0
    第一轮loss高到飞起，第二轮回归正常值，但一直不会收敛。（步子过大）
    
为弥补学习率过大，可以通过同时增大batch_size来控制loss。
    lr = 0.3, batch_size = 256 (原来的2倍)，收敛回复正常，但速度变慢，同样5个epoch之后，loss仍为default设定的3.5 倍。

    
GoogleNet：
比NiN深了很多，但每一个epoch的时间基本相等。
loss较NiN降低0.1， accuracy 升高。总epoch数5.
可能多增加几轮循环准确率会进一步提升。